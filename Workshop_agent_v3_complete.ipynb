{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f9f4220",
   "metadata": {},
   "source": [
    "# LangGraph Agent v3 — Complete (Analyze → Search → Enrich → Process)\n",
    "\n",
    "Final, self-contained agent with LLM decision to enrich or process.\n",
    "\n",
    "Quick guide:\n",
    "- State: messages, search_results, optimized_query, decision\n",
    "- Flow: analyze → search → decide → (enrich | process) → process → END\n",
    "- Observe prints: [ANALYZE], [SEARCH], [DECIDE], [ROUTE], [ENRICH], [PROCESS], [END]\n",
    "- The decision is persisted in state.decision and printed after run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c9b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional, Union, Literal\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cdc506",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchResult(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "    snippet: str\n",
    "    link: str\n",
    "    full_content: Optional[str] = None\n",
    "\n",
    "class AgentState(BaseModel):\n",
    "    messages: List[Union[HumanMessage, AIMessage, SystemMessage]] = Field(description=\"Chat history\")\n",
    "    search_results: List[SearchResult] = Field(default_factory=list)\n",
    "    optimized_query: Optional[str] = None\n",
    "    decision: Optional[Literal['enrich','process']] = None\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b0dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.7)\n",
    "search_engine = GoogleSearchAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6675ede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_full_content(url: str, max_length: int = 8000) -> str:\n",
    "    try:\n",
    "        docs = WebBaseLoader(web_paths=[url], header_template={\"User-Agent\":\"Mozilla/5.0\"}).load()\n",
    "        if docs and docs[0].page_content.strip():\n",
    "            t = docs[0].page_content\n",
    "            return t[:max_length] + ('…' if len(t) > max_length else '')\n",
    "        return '[Full content unavailable. Using snippet]'\n",
    "    except Exception as e:\n",
    "        return f'[Content unavailable: {e}]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f91f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query(state: AgentState):\n",
    "    print(\"[ANALYZE] Optimizing user query…\")\n",
    "    user_query = next((m.content for m in reversed(state.messages) if isinstance(m, HumanMessage)), \"\")\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You rewrite user queries into high-recall web search queries. Today's date is 2025-08-27. Preserve key entities and constraints (dates, names, locations). Add helpful synonyms if useful. Return ONLY the optimized query with no quotes or extra text.\"),\n",
    "        (\"human\", \"Optimize: {q}\")\n",
    "    ])\n",
    "    q = llm.invoke(prompt.format_messages(q=user_query)).content.strip().replace('\"','')\n",
    "    return {\"optimized_query\": q}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644de896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_search(state: AgentState):\n",
    "    print(\"[SEARCH] Running web search…\")\n",
    "    query = state.optimized_query or next((m.content for m in reversed(state.messages) if isinstance(m, HumanMessage)), \"\")\n",
    "    try:\n",
    "        raw = search_engine.results(query, num_results=5)\n",
    "    except Exception:\n",
    "        raw = [{\"title\":\"Search unavailable\",\"snippet\":f\"Could not search: {query}\",\"link\":\"https://example.com\"}]\n",
    "    results = [SearchResult(id=i+1,\n",
    "                            title=r.get('title','No title'),\n",
    "                            snippet=r.get('snippet', r.get('description','No content')),\n",
    "                            link=r.get('link', r.get('url','No link')))\n",
    "               for i, r in enumerate(raw)]\n",
    "    return {\"search_results\": results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911c0ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f2fa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_content(state: AgentState):\n",
    "    print(\"[ENRICH] Fetching and extracting relevant page content…\")\n",
    "    if not state.search_results:\n",
    "        return {}\n",
    "    user_query = next((m.content for m in state.messages if isinstance(m, HumanMessage)), 'Unknown')\n",
    "    for r in state.search_results:\n",
    "        raw = fetch_full_content(r.link)\n",
    "        print(raw)\n",
    "        if raw.startswith('[Content unavailable'):\n",
    "            r.full_content = raw\n",
    "        else:\n",
    "            print(\"-------\")\n",
    "            print(raw)\n",
    "            print(\"-------\")\n",
    "            p = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", \"From the page content, extract only information directly relevant to the user's query. Prefer concrete facts, figures, quotes, and timelines. Include brief inline citations with the source URL (e.g., [source: <URL>]). Ignore irrelevant sections.\"),\n",
    "                (\"human\", \"Query: {q}\\nTitle: {t}\\nURL: {u}\\nContent:\\n{c}\")\n",
    "            ])\n",
    "            r.full_content = llm.invoke(p.format_messages(q=user_query, t=r.title, u=r.link, c=raw)).content.strip()\n",
    "    return {\"search_results\": state.search_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39db589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_results(state: AgentState):\n",
    "    print(\"[PROCESS] Synthesizing final answer…\")\n",
    "    user_query = next((m.content for m in state.messages if isinstance(m, HumanMessage)), 'Unknown')\n",
    "    if state.search_results:\n",
    "        blocks = []\n",
    "        for r in state.search_results:\n",
    "            snippet = r.snippet or ''\n",
    "            detail = r.full_content\n",
    "            blocks.append(f\"Result {r.id} | {r.title}\\n{snippet}\\nSource: {r.link}\\n{detail}\")\n",
    "        res = '\\n'.join(blocks)\n",
    "    else:\n",
    "        res = 'No search results available.'\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        ('system', 'Answer the query using only the provided results. Be concise, precise, and cite URLs inline (e.g., [source: <URL>]). If evidence is weak or conflicting, say so and avoid speculation.'),\n",
    "        ('human', 'Query: {q}\\n\\nResults:\\n{res}')\n",
    "    ])\n",
    "    ans = llm.invoke(prompt.format_messages(q=user_query, res=res)).content.strip()\n",
    "    return {\"messages\": [AIMessage(content=ans)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4901f48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_to_search(state: AgentState) -> Literal['search']:\n",
    "    print(\"[ROUTE] analyze → search\")\n",
    "    return 'search'\n",
    "\n",
    "def decide_next(state: AgentState):\n",
    "    \"\"\"LLM-backed decision node that writes the decision into state.\"\"\"\n",
    "    print(\"[DECIDE] Choosing 'enrich' vs 'process'…\")\n",
    "\n",
    "    user_query = next((m.content for m in state.messages if isinstance(m, HumanMessage)), 'Unknown')\n",
    "\n",
    "    p = ChatPromptTemplate.from_messages([\n",
    "        ('system', \"Today is 2025-08-27. Reply ONLY 'enrich' or 'process'. Choose 'process' only if the current snippets clearly and fully answer the query with sufficient detail and sources. Otherwise choose 'enrich'. Be conservative.\"),\n",
    "        ('human', 'Query: {q}\\nSnippets:\\n' + '\\n'.join([f'- {r.title}: {r.snippet}' for r in state.search_results]))\n",
    "    ])\n",
    "    try:\n",
    "        d = llm.invoke(p.format_messages(q=user_query)).content.strip().lower()\n",
    "        decision = 'process' if ('process' in d and 'enrich' not in d) else 'enrich'\n",
    "    except Exception:\n",
    "        decision = 'enrich'\n",
    "    print(f\"[DECIDE] Decision: {decision}\")\n",
    "    return {\"decision\": decision}\n",
    "\n",
    "def route_by_decision(state: AgentState) -> Literal['enrich','process']:\n",
    "    nxt = state.decision or 'enrich'\n",
    "    print(f\"[ROUTE] decide → {nxt}\")\n",
    "    return nxt\n",
    "\n",
    "def is_done(state: AgentState) -> Literal['end']:\n",
    "    print(\"[END] process → END\")\n",
    "    return 'end'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da4e8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "complete_agent = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9d68bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test v3\n",
    "query = 'Who won the 2024 US presidential election?'\n",
    "state = AgentState(messages=[HumanMessage(content=query)])\n",
    "final_state = complete_agent.invoke(state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b667f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in final_state['messages']:\n",
    "    if isinstance(m, HumanMessage):\n",
    "        print(f'Human: {m.content}')\n",
    "    elif isinstance(m, AIMessage):\n",
    "        print(f'AI: {m.content}')\n",
    "    else:\n",
    "        print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a3a311",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
