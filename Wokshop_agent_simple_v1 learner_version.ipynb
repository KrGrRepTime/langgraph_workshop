{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad0db5ea",
   "metadata": {},
   "source": [
    "# Building AI Agents with LangGraph and LLMs\n",
    "\n",
    "This workshop guides you through creating an AI agent using LangGraph, a powerful framework for building stateful, reasoning-focused AI systems with explicit control flows.\n",
    "\n",
    "## What are AI Agents?\n",
    "\n",
    "AI agents are autonomous systems that perceive their environment, make decisions, and take actions to accomplish specific goals. Unlike simple language models that just respond to prompts, agents can:\n",
    "\n",
    "1. **Plan and Reason**: Break down complex tasks into logical steps\n",
    "2. **Use Tools**: Access external capabilities like search engines, databases, or APIs\n",
    "3. **Maintain State**: Remember context and previous actions across interactions\n",
    "4. **Make Decisions**: Choose appropriate actions based on their reasoning\n",
    "\n",
    "## What is LangGraph?\n",
    "\n",
    "LangGraph is a framework for building stateful, reasoning-focused AI systems with explicit control flows. It provides:\n",
    "\n",
    "1. **Structured Reasoning**: Create explicit multi-step reasoning processes\n",
    "2. **State Management**: Maintain and transform context between steps\n",
    "3. **Directed Workflows**: Design clear execution paths between components\n",
    "4. **Conditional Logic**: Implement decision-making capabilities\n",
    "\n",
    "In this workshop, we'll build a web search agent to demonstrate these concepts in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96009e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import TypedDict, List, Annotated, Literal\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal, Optional, Union\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import json\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# LangGraph imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI  # Import Gemini\n",
    "from langchain_openai import ChatOpenAI  # Alternative: OpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# For web search\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper  # Option 1\n",
    "from langchain_community.utilities import SerpAPIWrapper  # Option 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281be10e",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "We'll start by importing the necessary libraries for our agent. These include:\n",
    "\n",
    "- **LangGraph**: For creating our agent's reasoning workflow\n",
    "- **LangChain**: For working with LLMs and tools\n",
    "- **Pydantic**: For creating strongly typed data models\n",
    "- **Web Search APIs**: To give our agent the ability to search the internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5158692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if API keys are set\n",
    "print(\"Google API key is\", \"set\" if os.environ.get(\"GOOGLE_API_KEY\") else \"not set\")\n",
    "print(\"OpenAI API key is\", \"set\" if os.environ.get(\"OPENAI_API_KEY\") else \"not set\")\n",
    "print(\"Google CSE ID is\", \"set\" if os.environ.get(\"GOOGLE_CSE_ID\") else \"not set\")\n",
    "print(\"SerpAPI key is\", \"set\" if os.environ.get(\"SERPAPI_API_KEY\") else \"not set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f683f782",
   "metadata": {},
   "source": [
    "## Checking API Keys\n",
    "\n",
    "Before proceeding, we need to verify that the necessary API keys are available in our environment:\n",
    "\n",
    "- **Google API Key**: For Gemini LLM access or Google search capabilities\n",
    "- **OpenAI API Key**: Alternative LLM provider\n",
    "- **Google CSE ID**: For Google Custom Search Engine\n",
    "- **SerpAPI Key**: Alternative search engine API\n",
    "\n",
    "The agent will use these APIs to access knowledge beyond its training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchResult(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "    snippet: str \n",
    "    link: str\n",
    "    relevance_score: Optional[float] = None\n",
    "    full_content: Optional[str] = None\n",
    "    \n",
    "class AgentState(BaseModel):\n",
    "    messages: List[Union[HumanMessage, AIMessage, SystemMessage]] = Field(description=\"The chat history\")\n",
    "    search_results: List[SearchResult] = Field(default_factory=list, description=\"The results from the web search\")\n",
    "    optimized_query: Optional[str] = Field(default=None, description=\"The LLM-optimized search query\")\n",
    "    \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b5c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_question(state: AgentState) -> AgentState:\n",
    "    \"\"\"Entry point: Analyze the user's question and answer with the knowledge you have.\"\"\"\n",
    "    try:\n",
    "        # Get the latest user message\n",
    "        last_user_message = next(\n",
    "            (msg.content for msg in reversed(state.messages) if isinstance(msg, HumanMessage)),\n",
    "            \"\"\n",
    "        )\n",
    "\n",
    "        # INSTRUCTOR_NOTE: This prompt can be improved by students during the workshop\n",
    "        answer_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"\n",
    "            You are a knowledgeable sports analyst specializing in basketball.\n",
    "            \n",
    "            When asked about sports games or events:\n",
    "            1. Always provide specific information you know about the game/team - including dates, scores, locations, etc.\n",
    "            2. Cite notable performances and key moments from games you're familiar with\n",
    "            3. If you're uncertain of the most recent result, provide information about the last game you're confident about\n",
    "            4. Include the actual score and match details for any game you reference\n",
    "            \n",
    "            IMPORTANT: Even without real-time data, you should provide factual information about \n",
    "            past games and specifics you know about, not just generic responses.\n",
    "            \"\"\"),\n",
    "            (\"human\", \"Answer this query as specifically as possible: {query}\")\n",
    "        ])\n",
    "        # Invoke the LLM \n",
    "        question_response = llm.invoke(\n",
    "            answer_prompt.format_messages(\n",
    "                query=last_user_message\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Store the response in the state object\n",
    "        state.messages.append(AIMessage(content=question_response.content))\n",
    "        \n",
    "        return state\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Just return the original state if there was an error\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba46817",
   "metadata": {},
   "source": [
    "## Basic Agent Implementation\n",
    "\n",
    "Here we implement our first agent function that analyzes user questions and provides responses.\n",
    "\n",
    "### Understanding Node Functions in LangGraph\n",
    "\n",
    "In LangGraph, **nodes** are functions that:\n",
    "1. Take the current state as input\n",
    "2. Perform some processing or reasoning\n",
    "3. Return a modified state as output\n",
    "\n",
    "Our `analyze_question` function implements this pattern by:\n",
    "1. Extracting the user's message from the state\n",
    "2. Creating a prompt for the LLM to answer the question\n",
    "3. Invoking the LLM to generate a response\n",
    "4. Adding the response to the message history in the state\n",
    "5. Returning the updated state\n",
    "\n",
    "This simple approach handles basic questions but has limitations - the LLM can only answer based on its training data, without access to current information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7912de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTOR_COMPLETE_THIS_IN_WORKSHOP\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc127c7",
   "metadata": {},
   "source": [
    "## Initializing the Language Model\n",
    "\n",
    "We'll use Google's Gemini model as our reasoning engine. LLMs in agent systems serve multiple roles:\n",
    "\n",
    "1. **Core Reasoning**: Analyzing problems and planning solutions\n",
    "2. **Tool Use**: Deciding when and how to use available tools\n",
    "3. **Information Processing**: Extracting and synthesizing knowledge\n",
    "4. **Response Generation**: Creating human-readable outputs\n",
    "\n",
    "The choice of LLM impacts the agent's capabilities, with factors like context window size, reasoning abilities, and tool use proficiency all playing important roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73276d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_done(state: AgentState) -> Literal[\"end\"]:\n",
    "    \"\"\"Simple function to signal the end of the workflow\"\"\"\n",
    "    return \"end\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364f8289",
   "metadata": {},
   "source": [
    "## Workflow Control Functions\n",
    "\n",
    "LangGraph uses specialized functions to control the flow of execution through the agent graph.\n",
    "\n",
    "Here we define `is_done`, a simple function that always returns \"end\" to signal that processing is complete and the workflow should terminate. In more complex agents, this function could implement conditional logic to determine whether more processing is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d0fcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTOR_COMPLETE_THIS_IN_WORKSHOP\n",
    "# Create the graph with our extended functions\n",
    "workflow_v1 = StateGraph(AgentState)\n",
    "\n",
    "# Add our nodes\n",
    "workflow_v1.add_node(\"analyze\", analyze_question)  # New entry point\n",
    "\n",
    "# Connect process to end\n",
    "workflow_v1.add_conditional_edges(\n",
    "    \"analyze\",\n",
    "    is_done,\n",
    "    {\n",
    "        \"end\": END  # Always end after processing\n",
    "    }\n",
    ")\n",
    "\n",
    "# Set the entry point to our new analysis node\n",
    "workflow_v1.set_entry_point(\"analyze\")\n",
    "\n",
    "# Compile the graph\n",
    "agent = workflow_v1.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b7c92c",
   "metadata": {},
   "source": [
    "## Creating the Graph Structure\n",
    "\n",
    "Now we'll build our first graph structure using LangGraph's `StateGraph`. This creates the workflow that our agent will follow.\n",
    "\n",
    "### Key Components of StateGraph:\n",
    "\n",
    "1. **Nodes**: Functions that process the state (`analyze_question`)\n",
    "2. **Edges**: Connections between nodes, defining the flow of execution\n",
    "3. **Conditional Edges**: Paths that depend on the state or routing functions\n",
    "4. **Entry Point**: The starting node when the graph is executed\n",
    "\n",
    "When compiled, the graph becomes a runnable agent that can process user queries through the defined workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02fcb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who won US presidential election in 2024?\"\n",
    "initial_state = AgentState(messages=[HumanMessage(content=query)])\n",
    "\n",
    "# Run the agent\n",
    "final_state = agent.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51509e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the conversation\n",
    "for message in final_state['messages']:\n",
    "    if isinstance(message, HumanMessage):\n",
    "        print(f\"Human: {message.content}\")\n",
    "    elif isinstance(message, AIMessage):\n",
    "        print(f\"AI: {message.content}\")\n",
    "    else:\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4010a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's implement our search-enabled agent components\n",
    "\n",
    "# Choose your search tool implementation - uncomment the one you want to use\n",
    "# Option 1: Using Google Search API\n",
    "search_engine = GoogleSearchAPIWrapper(k=3)  # Get top 3 results\n",
    "\n",
    "# Task Start\n",
    "def analyze_query(state: AgentState) -> AgentState:\n",
    "    \"\"\"Entry point: Analyze the user's question and optimize the search query\"\"\"\n",
    "    try:\n",
    "        # Get the latest user message\n",
    "        last_user_message = next(\n",
    "            (msg.content for msg in reversed(state.messages) if isinstance(msg, HumanMessage)),\n",
    "            \"\"\n",
    "        )\n",
    "        \n",
    "        # Create your own prompt for query optimization\n",
    "        optimization_prompt = ChatPromptTemplate.from_messages([\n",
    "            # TODO: Write a system prompt that helps optimize search queries\n",
    "            (\"system\", \"\"\"\n",
    "\n",
    "       \"\"\"),\n",
    "            (\"human\", \"Optimize this search query: {query}\")\n",
    "        ])\n",
    "        \n",
    "        # Invoke the LLM to optimize the query\n",
    "        optimization_response = llm.invoke(\n",
    "            optimization_prompt.format_messages(\n",
    "                query=last_user_message\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Store the optimized query directly in the state object\n",
    "        state.optimized_query = optimization_response.content.strip()\n",
    "        \n",
    "        return state\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Just return the original state if there was an error\n",
    "        return state\n",
    "# Task END\n",
    "\n",
    "def perform_search(state: AgentState) -> AgentState:\n",
    "    \"\"\"Second node: Perform the web search based on optimized query\"\"\"\n",
    "    try:\n",
    "        # Use the optimized query directly from state attribute, or fall back to original query\n",
    "        if state.optimized_query:\n",
    "            search_query = state.optimized_query\n",
    "        else:\n",
    "            # Fallback to original query if optimization failed\n",
    "            search_query = next(\n",
    "                (msg.content for msg in reversed(state.messages) if isinstance(msg, HumanMessage)),\n",
    "                \"\"\n",
    "            )\n",
    "        \n",
    "        # Perform the search\n",
    "        try:\n",
    "            # Try to get search results\n",
    "            search_results = search_engine.results(search_query, num_results=3)\n",
    "        except Exception as search_error:\n",
    "            # Handle common API errors\n",
    "            print(f\"Search API error: {str(search_error)}\")\n",
    "            # Create a dummy result to allow the workshop to continue\n",
    "            search_results = [{\n",
    "                'title': 'Search temporarily unavailable',\n",
    "                'snippet': f'Could not retrieve search results for: {search_query}. '\n",
    "                          f'Please check your API keys and configuration.',\n",
    "                'link': 'https://example.com'\n",
    "            }]\n",
    "        \n",
    "        # Convert to our Pydantic model\n",
    "        results_list = []\n",
    "        for i, result in enumerate(search_results, 1):\n",
    "            results_list.append(\n",
    "                SearchResult(\n",
    "                    id=i,\n",
    "                    title=result.get('title', 'No title'),\n",
    "                    snippet=result.get('snippet', result.get('description', 'No content')),\n",
    "                    link=result.get('link', result.get('url', 'No link'))\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Update state with search results\n",
    "        state.search_results = results_list\n",
    "        return state\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during search: {str(e)}\")\n",
    "        # Return the original state with an empty search results list\n",
    "        state.search_results = []\n",
    "        return state\n",
    "\n",
    "\n",
    "def process_results(state: AgentState) -> AgentState:\n",
    "    \"\"\"Fourth node: Process the query and search results to generate a response\"\"\"\n",
    "    try:\n",
    "        # Get current date\n",
    "        from datetime import datetime\n",
    "        current_date = datetime.now().strftime(\"%B %d, %Y\")\n",
    "        \n",
    "        # Format search results for the LLM\n",
    "        search_results_text = \"\"\n",
    "        \n",
    "        if not state.search_results:\n",
    "            search_results_text = \"No search results available. The search API may be unavailable.\"\n",
    "        else:\n",
    "            for result in state.search_results:\n",
    "                search_results_text += f\"Result {result.id}:\\nTitle: {result.title}\\nSnippet: {result.snippet}\\nSource: {result.link}\\n\\n\"\n",
    "                \n",
    "                if hasattr(result, 'full_content') and result.full_content:\n",
    "                    search_results_text += f\"\\nExtracted Content:\\n{result.full_content}\\n\\n\"\n",
    "                \n",
    "        # Create a prompt template for synthesis\n",
    "        synthesis_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", f\"\"\"You are a basketball sports analyst answering questions based on search results.\n",
    "            Today's date is {current_date}.\n",
    "            \"\"\"),\n",
    "            (\"human\", \"User query: {original_query}\\n\\nSearch results:\\n{search_results}\"),\n",
    "        ])\n",
    "        \n",
    "        # Get the original query\n",
    "        original_query = next(\n",
    "            (msg.content for msg in state.messages if isinstance(msg, HumanMessage)),\n",
    "            \"Unknown query\"\n",
    "        )\n",
    "        \n",
    "        # Generate a response\n",
    "        response = llm.invoke(\n",
    "            synthesis_prompt.format_messages(\n",
    "                original_query=original_query,\n",
    "                search_results=search_results_text\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Update the state with the assistant's response\n",
    "        state.messages.append(AIMessage(content=response.content))\n",
    "        return state\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Add a basic response if processing fails\n",
    "        state.messages.append(AIMessage(content=\"I apologize, but I encountered an issue processing your request. Please try again.\"))\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dcce9b",
   "metadata": {},
   "source": [
    "## Enhanced Agent with Web Search Capabilities\n",
    "\n",
    "To make our agent more powerful, let's add web search capabilities. This transforms it from a system limited by training data to one that can access current information.\n",
    "\n",
    "### Multi-Step Reasoning Architecture\n",
    "\n",
    "We'll implement a more sophisticated workflow with multiple specialized components:\n",
    "\n",
    "1. **Query Analysis**: Optimize the user's question for search effectiveness\n",
    "2. **Search Execution**: Retrieve relevant information from the web\n",
    "3. **Result Processing**: Synthesize information into a coherent response\n",
    "\n",
    "This demonstrates a key pattern in LangGraph: breaking complex tasks into focused subtasks with dedicated reasoning at each step. By specializing each component, we get better performance than trying to accomplish everything in a single prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aa4a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTOR_COMPLETE_THIS_IN_WORKSHOP\n",
    "def route_to_search(state: AgentState) -> Literal[\"search\"]:\n",
    "    \"\"\"Route to the search node after query analysis\"\"\"\n",
    "    return \"search\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d76ff43",
   "metadata": {},
   "source": [
    "## Routing Functions in LangGraph\n",
    "\n",
    "Routing functions determine the flow of execution through the graph. They analyze the current state and return a string identifier that specifies which node should be executed next.\n",
    "\n",
    "Here, `route_to_search` always returns \"search\" - creating a simple linear flow from analysis to search. In more complex agents, these functions could implement sophisticated decision logic based on the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a528a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT_IMPLEMENTATION_SECTION_BEGIN\n",
    "# Create the graph with our extended functions\n",
    "workflow_v2 = StateGraph(AgentState)\n",
    "\n",
    "# Set the entry point to our new analysis node\n",
    "workflow_v2.set_entry_point(\"analyze\")\n",
    "\n",
    "# Compile the graph\n",
    "agent_v2 = workflow_v2.compile()\n",
    "# STUDENT_IMPLEMENTATION_SECTION_END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b08b774",
   "metadata": {},
   "source": [
    "## Designing the Enhanced Agent Graph\n",
    "\n",
    "Let's create a more advanced agent graph with our multi-step workflow.\n",
    "\n",
    "### LangGraph Workflow Design\n",
    "\n",
    "When designing LangGraph workflows, consider these principles:\n",
    "\n",
    "1. **Single Responsibility**: Each node should perform a specific, focused task\n",
    "2. **State Transformation**: Nodes transform the state in predictable ways\n",
    "3. **Explicit Decision Points**: Use conditional edges to represent decision logic\n",
    "4. **Error Handling**: Account for failures and edge cases\n",
    "\n",
    "Our enhanced agent follows this pattern with explicit nodes for query analysis, search, and result processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e1e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who won US presidential election in 2024??\"\n",
    "initial_state = AgentState(messages=[HumanMessage(content=query)])\n",
    "\n",
    "# Run the agent\n",
    "final_state = agent_v2.invoke(initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d80ac1",
   "metadata": {},
   "source": [
    "## Testing Our Enhanced Agent\n",
    "\n",
    "Let's test our search-enabled agent with a specific query. The execution will follow these steps:\n",
    "\n",
    "1. Initial state with user query is created\n",
    "2. Query analysis optimizes the search terms\n",
    "3. Search retrieves relevant information\n",
    "4. Processing synthesizes a comprehensive response\n",
    "\n",
    "This demonstrates the multi-step reasoning pattern in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda4b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell shows the full state object - you can run it to explore the complete state\n",
    "# final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75770e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the conversation\n",
    "for message in final_state['messages']:\n",
    "    if isinstance(message, HumanMessage):\n",
    "        print(f\"Human: {message.content}\")\n",
    "    elif isinstance(message, AIMessage):\n",
    "        print(f\"AI: {message.content}\")\n",
    "    else:\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd0e04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT_IMPLEMENTATION_SECTION_BEGIN\n",
    "# Choose your search tool implementation - uncomment the one you want to use\n",
    "\n",
    "# Option 1: Using Google Search API\n",
    "search_engine = GoogleSearchAPIWrapper(k=3)  # Get top 3 results\n",
    "def analyze_query(state: AgentState) -> AgentState:\n",
    "\n",
    "    return state\n",
    "\n",
    "def fetch_full_content(url: str, max_length: int = 8000) -> str:\n",
    "    \"\"\"Fetch the full content of a webpage and return a truncated version with better error handling\"\"\"\n",
    "    try:\n",
    "        # Add user-agent to avoid some basic blocking\n",
    "        loader = WebBaseLoader(\n",
    "            web_paths=[url],\n",
    "            header_template={\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"}\n",
    "        )\n",
    "        \n",
    "        # Try to load the content\n",
    "        docs = loader.load()\n",
    "        \n",
    "        # Check if we got any content\n",
    "        if docs and docs[0].page_content.strip():\n",
    "            content = docs[0].page_content\n",
    "            \n",
    "            # Truncate to avoid token limits\n",
    "            return content[:max_length] + (\"...\" if len(content) > max_length else \"\")\n",
    "        else:\n",
    "            # Try an alternative approach - extract from the snippet directly\n",
    "            print(f\"No content found using WebBaseLoader for {url}, using fallback method\")\n",
    "            \n",
    "            # For sports sites, often the snippet contains the most relevant information\n",
    "            # Return a message indicating we're using the snippet from search results\n",
    "            return \"[Full content unavailable. Using search result snippet instead]\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching content from {url}: {str(e)}\")\n",
    "        \n",
    "        # More detailed error handling\n",
    "        if \"timeout\" in str(e).lower():\n",
    "            return \"[Content unavailable: Connection timed out]\"\n",
    "        elif \"403\" in str(e) or \"forbidden\" in str(e).lower():\n",
    "            return \"[Content unavailable: Access forbidden - website may block scraping]\"\n",
    "        elif \"404\" in str(e) or \"not found\" in str(e).lower():\n",
    "            return \"[Content unavailable: Page not found]\"\n",
    "        else:\n",
    "            return f\"[Content unavailable: {str(e)}]\"\n",
    "\n",
    "def perform_search(state: AgentState) -> AgentState:\n",
    "    \"\"\"Second node: Perform the web search based on optimized query\"\"\"\n",
    "    try:\n",
    "        # Use the optimized query directly from state attribute, or fall back to original query\n",
    "        if state.optimized_query:\n",
    "            search_query = state.optimized_query\n",
    "        else:\n",
    "            # Fallback to original query if optimization failed\n",
    "            search_query = next(\n",
    "                (msg.content for msg in reversed(state.messages) if isinstance(msg, HumanMessage)),\n",
    "                \"\"\n",
    "            )\n",
    "        \n",
    "        # Perform the search\n",
    "        try:\n",
    "            # Try to get search results\n",
    "            search_results = search_engine.results(search_query, num_results=5)\n",
    "        except Exception as search_error:\n",
    "            # Handle common API errors\n",
    "            print(f\"Search API error: {str(search_error)}\")\n",
    "            # Create a dummy result to allow the workshop to continue\n",
    "            search_results = [{\n",
    "                'title': 'Search temporarily unavailable',\n",
    "                'snippet': f'Could not retrieve search results for: {search_query}. '\n",
    "                          f'Please check your API keys and configuration.',\n",
    "                'link': 'https://example.com'\n",
    "            }]\n",
    "        \n",
    "        # Convert to our Pydantic model\n",
    "        results_list = []\n",
    "        for i, result in enumerate(search_results, 1):\n",
    "            results_list.append(\n",
    "                SearchResult(\n",
    "                    id=i,\n",
    "                    title=result.get('title', 'No title'),\n",
    "                    snippet=result.get('snippet', result.get('description', 'No content')),\n",
    "                    link=result.get('link', result.get('url', 'No link'))\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Update state with search results\n",
    "        state.search_results = results_list\n",
    "        return state\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during search: {str(e)}\")\n",
    "        # Return the original state with an empty search results list\n",
    "        state.search_results = []\n",
    "        return state\n",
    "\n",
    "def enrich_content(state: AgentState) -> AgentState:\n",
    "    \"\"\"Third node: Use LLM to fetch and extract relevant information from top search results\"\"\"\n",
    "    try:\n",
    "        # Get the original query to provide context\n",
    "        original_query = next(\n",
    "            (msg.content for msg in state.messages if isinstance(msg, HumanMessage)),\n",
    "            \"Unknown query\"\n",
    "        )\n",
    "        \n",
    "        # Only enrich the first result to keep it simple for workshop purposes\n",
    "        if state.search_results and len(state.search_results) > 0:\n",
    "            for i in range(min(2, len(state.search_results))):\n",
    "                # Use the first search result\n",
    "                result = state.search_results[i]\n",
    "                \n",
    "                # First, fetch the raw content\n",
    "                raw_content = fetch_full_content(result.link)\n",
    "                \n",
    "                # If content was successfully fetched, use LLM to extract and summarize\n",
    "                if raw_content and not raw_content.startswith(\"[Content unavailable\"):\n",
    "                    # Create a prompt for the LLM to extract and summarize\n",
    "                    extraction_prompt = ChatPromptTemplate.from_messages([\n",
    "                        (\"system\", \"\"\"You are an expert at extracting and summarizing relevant information from web content.\n",
    "                        \n",
    "                        Given a web page's content and a user query, your task is to:\n",
    "                        1. Identify the most relevant sections to the query\n",
    "                        2. Extract all the key facts, data points, and insights\n",
    "                        3. Organize the information in a clear, coherent structure\n",
    "                        4. Summarize the content while preserving accuracy\n",
    "                        5. Include any relevant technical details, statistics, or examples\n",
    "                        \n",
    "                        Focus on extracting ONLY information that addresses the user's query.\n",
    "                        Structure your summary with appropriate headings and bullet points if needed.\n",
    "                        Limit your response to the most important information (around 500-800 words).\"\"\"),\n",
    "                        (\"human\", \"User query: {query}\\n\\nWeb page title: {title}\\nURL: {url}\\n\\nContent:\\n{content}\")\n",
    "                    ])\n",
    "                    \n",
    "                    # Invoke the LLM to extract and summarize\n",
    "                    extraction_response = llm.invoke(\n",
    "                        extraction_prompt.format_messages(\n",
    "                            query=original_query,\n",
    "                            title=result.title,\n",
    "                            url=result.link,\n",
    "                            content=raw_content\n",
    "                        )\n",
    "                    )\n",
    "                    \n",
    "                    # Store the LLM's extracted and summarized content\n",
    "                    result.full_content = extraction_response.content\n",
    "                else:\n",
    "                    # If content couldn't be fetched, note this in the result\n",
    "                    result.full_content = \"[Content could not be accessed or processed]\"\n",
    "            \n",
    "        return state\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Add a note about the failure\n",
    "        if state.search_results and len(state.search_results) > 0:\n",
    "            state.search_results[0].full_content = f\"[Error during content enrichment: {str(e)}]\"\n",
    "        return state\n",
    "\n",
    "def process_results(state: AgentState) -> AgentState:\n",
    "    \"\"\"Fourth node: Process the query and search results to generate a response\"\"\"\n",
    "    try:\n",
    "\n",
    "        return state\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Add a basic response if processing fails\n",
    "        state.messages.append(AIMessage(content=\"I apologize, but I encountered an issue processing your request. Please try again.\"))\n",
    "        return state\n",
    "# STUDENT_IMPLEMENTATION_SECTION_END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620d0ee4",
   "metadata": {},
   "source": [
    "## Advanced Agent with Content Enrichment\n",
    "\n",
    "### LLMs as Multi-Purpose Components\n",
    "\n",
    "In modern agent architecture, LLMs serve multiple roles beyond final output generation. Our agent demonstrates this with multiple LLM-powered processes:\n",
    "\n",
    "1. **Query Analysis and Optimization**: Before searching, the LLM analyzes the user's question to create more effective search terms.\n",
    "\n",
    "2. **Content Enrichment**: After retrieving search results, the LLM processes web content to extract the most relevant information.\n",
    "\n",
    "3. **Response Synthesis**: Finally, the LLM combines all information into a coherent, comprehensive answer.\n",
    "\n",
    "This multi-stage approach is far more effective than attempting to accomplish everything in a single LLM call. The `fetch_full_content` function enables a key capability: retrieving detailed information from web pages to augment the limited snippets provided by search engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87332d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTOR_COMPLETE_THIS_IN_WORKSHOP\n",
    "def route_to_search(state: AgentState) -> Literal[\"search\"]:\n",
    "    \"\"\"Route to the search node after query analysis\"\"\"\n",
    "    return \"search\"\n",
    "\n",
    "def should_enrich_content(state: AgentState) -> Literal[\"enrich\", \"process\"]:\n",
    "    \"\"\"Decide whether to enrich content or proceed to processing\"\"\"\n",
    "    # For simplicity in this workshop, we'll always enrich content\n",
    "    return \"enrich\"\n",
    "\n",
    "def is_done(state: AgentState) -> Literal[\"end\"]:\n",
    "    \"\"\"Simple function to signal the end of the workflow\"\"\"\n",
    "    return \"end\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852e1769",
   "metadata": {},
   "source": [
    "## Dynamic Flow Control in LangGraph\n",
    "\n",
    "These functions control the flow of execution in our agent graph:\n",
    "\n",
    "1. **route_to_search**: Directs the flow from query analysis to search\n",
    "2. **should_enrich_content**: Determines whether to enrich content or proceed directly to processing\n",
    "3. **is_done**: Signals the end of the workflow\n",
    "\n",
    "Conditional routing enables dynamic decision-making in LangGraph. While our current functions are simple, they could implement complex logic in production systems, such as:\n",
    "\n",
    "- Skipping search for questions the LLM can answer directly\n",
    "- Enriching only the most promising search results\n",
    "- Triggering additional research for insufficient information\n",
    "\n",
    "These decision points are where the \"agent\" aspect truly emerges - the system adapts its behavior based on the specific context and needs of each query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ab84df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT_IMPLEMENTATION_SECTION_BEGIN\n",
    "# Create the graph with our extended functions\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "\n",
    "# Compile the graph\n",
    "agent = workflow.compile()\n",
    "# STUDENT_IMPLEMENTATION_SECTION_END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4729039",
   "metadata": {},
   "source": [
    "## Building the Complete Agent Workflow\n",
    "\n",
    "### LangGraph Architecture Patterns\n",
    "\n",
    "Our complete agent implementation demonstrates key LangGraph architecture patterns:\n",
    "\n",
    "1. **Graph-Based Orchestration**: Explicit workflow with clearly defined stages\n",
    "2. **Typed State Management**: Structured state passed and transformed between nodes\n",
    "3. **Conditional Branching**: Decision points that determine execution flow\n",
    "4. **Component Specialization**: Each node focused on a specific task\n",
    "\n",
    "The workflow follows this pattern:\n",
    "```\n",
    "[Analyze] → [Search] → [Enrich] → [Process] → END\n",
    "```\n",
    "\n",
    "This approach offers several advantages over monolithic agent designs:\n",
    "\n",
    "- **Maintainability**: Components can be improved independently\n",
    "- **Transparency**: Reasoning steps can be traced and debugged\n",
    "- **Specialization**: Prompts optimized for specific subtasks\n",
    "- **Robustness**: Failure in one component doesn't break the entire system\n",
    "\n",
    "These patterns are essential for building production-grade agent systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0b8ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with a query about current NBA playoffs\n",
    "query = \"Who won US presidential election in 2024?\"\n",
    "initial_state = AgentState(messages=[HumanMessage(content=query)])\n",
    "\n",
    "# Run the agent\n",
    "final_state = agent.invoke(initial_state)\n",
    "\n",
    "# Display the conversation\n",
    "for message in final_state['messages']:\n",
    "    if isinstance(message, HumanMessage):\n",
    "        print(f\"Human: {message.content}\")\n",
    "    elif isinstance(message, AIMessage):\n",
    "        print(f\"AI: {message.content}\")\n",
    "    else:\n",
    "        print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d699aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell shows the final state object - you can run it to explore the complete state\n",
    "# final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7256c8f",
   "metadata": {},
   "source": [
    "## Putting It All Together: Testing the Complete Agent\n",
    "\n",
    "### The Complete Agent Experience\n",
    "\n",
    "Let's test our complete agent with a specific query. The execution will demonstrate the full workflow:\n",
    "\n",
    "1. **Query Analysis**: Optimizing the search terms\n",
    "2. **Search Execution**: Retrieving information from the web\n",
    "3. **Content Enrichment**: Processing detailed content from web pages\n",
    "4. **Response Synthesis**: Creating a comprehensive answer\n",
    "\n",
    "### Beyond This Workshop\n",
    "\n",
    "This workshop demonstrates fundamental patterns in LangGraph agent development, but many advanced capabilities are possible:\n",
    "\n",
    "1. **Memory and Long-term Context**: Storing and retrieving information across sessions\n",
    "2. **Multi-agent Systems**: Coordinating multiple specialized agents\n",
    "3. **Planning and Reflection**: Adding meta-cognitive abilities to agents\n",
    "4. **Feedback and Learning**: Improving agent performance over time\n",
    "5. **User Adaptation**: Customizing behavior based on user preferences\n",
    "\n",
    "By understanding the core concepts presented here, you're well-prepared to explore these more advanced topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224faacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the final state\n",
    "final_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab52d3e8",
   "metadata": {},
   "source": [
    "## Workshop Summary and Key Takeaways\n",
    "\n",
    "### Core Concepts Covered\n",
    "\n",
    "1. **AI Agent Architecture**\n",
    "   - Components, state management, and decision making\n",
    "   - Structuring complex reasoning workflows\n",
    "\n",
    "2. **LangGraph Framework**\n",
    "   - StateGraph, nodes, edges, and conditional routing\n",
    "   - State transformation pattern\n",
    "\n",
    "3. **Multi-stage LLM Reasoning**\n",
    "   - Specialized prompting for subtasks\n",
    "   - Processing and synthesis patterns\n",
    "\n",
    "4. **Enhanced Agent Capabilities**\n",
    "   - Web search integration\n",
    "   - Content enrichment\n",
    "   - Response synthesis\n",
    "\n",
    "### Next Steps for Learning\n",
    "\n",
    "1. **Experiment with different LLMs** to compare their reasoning capabilities\n",
    "2. **Add more tools** to your agent (e.g., calculators, weather APIs)\n",
    "3. **Implement persistent memory** to maintain context across sessions\n",
    "4. **Create testing scenarios** to evaluate your agent's performance\n",
    "5. **Build domain-specific agents** for particular use cases\n",
    "\n",
    "By mastering these concepts, you're well on your way to developing sophisticated AI agents that can assist with complex, multi-step tasks while maintaining context and making intelligent decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e19422",
   "metadata": {},
   "source": [
    "## Structured Output Parsing with Pydantic\n",
    "\n",
    "One common challenge in agent development is ensuring consistent, well-structured outputs from LLM calls. Let's demonstrate how to use LangChain's PydanticOutputParser to enforce structured outputs in our agent nodes.\n",
    "\n",
    "This approach provides several benefits:\n",
    "\n",
    "1. **Type Safety**: Ensures outputs match your expected schema\n",
    "2. **Error Handling**: Gracefully handles parsing failures\n",
    "3. **Prompt Engineering**: Automatically generates format instructions\n",
    "4. **Consistency**: Standardizes outputs across different LLM providers\n",
    "\n",
    "Let's implement a version of our query analysis component with structured outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab488ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT_IMPLEMENTATION_SECTION_BEGIN\n",
    "# Import structured output parsing components\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "# Note: We're importing directly from pydantic rather than langchain_core.pydantic_v1\n",
    "from typing import List, Optional\n",
    "# STUDENT_IMPLEMENTATION_SECTION_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca48c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT_IMPLEMENTATION_SECTION_BEGIN\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    \"\"\"Schema for a structured search query output\"\"\"\n",
    "    original_query: str = Field(description=\"The original user query\")\n",
    "    optimized_query: str = Field(description=\"The optimized search query\")\n",
    "    key_terms: List[str] = Field(description=\"Key terms extracted from the query\")\n",
    "    query_intent: str = Field(description=\"The interpreted intent of the user's query\")\n",
    "    information_need: str = Field(description=\"What specific information the user is looking for\")\n",
    "    \n",
    "    # Using model_validator instead of validator for Pydantic v2\n",
    "    @validator('key_terms')\n",
    "    def validate_key_terms(cls, key_terms):\n",
    "        \"\"\"Validate that key terms are not empty and are well-formed.\"\"\"\n",
    "        if not key_terms or len(key_terms) < 1:\n",
    "            raise ValueError(\"Must identify at least one key term\")\n",
    "        return key_terms\n",
    "        \n",
    "    # Add model_config for Pydantic v2\n",
    "    model_config = {\n",
    "        \"json_schema_extra\": {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"original_query\": \"What was the last Kauno Zalgiris basketball game?\",\n",
    "                    \"optimized_query\": \"Kauno Zalgiris latest basketball game result\",\n",
    "                    \"key_terms\": [\"Kauno Zalgiris\", \"basketball\", \"latest game\"],\n",
    "                    \"query_intent\": \"finding information about recent sports event\",\n",
    "                    \"information_need\": \"details about the most recent basketball game played by Kauno Zalgiris\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "# STUDENT_IMPLEMENTATION_SECTION_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef329e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT_IMPLEMENTATION_SECTION_BEGIN\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Optional\n",
    "\n",
    "class StructuredResponse(BaseModel):\n",
    "    \"\"\"Schema for a structured agent response\"\"\"\n",
    "    answer: str = Field(description=\"The direct answer to the user's question\")\n",
    "    sources: List[str] = Field(description=\"List of sources used to formulate the answer\")\n",
    "    confidence_score: float = Field(description=\"Confidence score between 0.0 and 1.0\")\n",
    "    related_topics: Optional[List[str]] = Field(None, description=\"Related topics the user might be interested in\")\n",
    "    \n",
    "    @validator('confidence_score')\n",
    "    def validate_confidence(cls, score):\n",
    "        \"\"\"Validate confidence score is between 0 and 1.\"\"\"\n",
    "        if score < 0 or score > 1:\n",
    "            raise ValueError(\"Confidence score must be between 0.0 and 1.0\")\n",
    "        return score\n",
    "        \n",
    "    # Add model_config for Pydantic v2\n",
    "    model_config = {\n",
    "        \"json_schema_extra\": {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"answer\": \"Kauno Zalgiris played their last game against Barcelona on March 12, 2023, losing 88-73.\",\n",
    "                    \"sources\": [\"https://example.com/sports/basketball/zalgiris-barcelona-2023\"],\n",
    "                    \"confidence_score\": 0.9,\n",
    "                    \"related_topics\": [\"Zalgiris EuroLeague standing\", \"Upcoming Zalgiris games\"]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "# STUDENT_IMPLEMENTATION_SECTION_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7b384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT_IMPLEMENTATION_SECTION_BEGIN\n",
    "def analyze_query_structured(state: AgentState) -> AgentState:\n",
    "    \"\"\"Enhanced entry point: Analyze the user's question with structured output\"\"\"\n",
    "    try:\n",
    "        # Get the latest user message\n",
    "        last_user_message = next(\n",
    "            (msg.content for msg in reversed(state.messages) if isinstance(msg, HumanMessage)),\n",
    "            \"\"\n",
    "        )\n",
    "        \n",
    "        # Setup our Pydantic parser\n",
    "        parser = PydanticOutputParser(pydantic_object=SearchQuery)\n",
    "        \n",
    "        # Format instructions tell the LLM how to structure its response\n",
    "        format_instructions = parser.get_format_instructions()\n",
    "        \n",
    "        # Create a prompt for query optimization with structured output\n",
    "        optimization_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \n",
    "             \"\"\"You are a research assistant synthesizing information from search results.\n",
    "            \n",
    "            Synthesize all information into a comprehensive, well-structured response that:\n",
    "            - Directly answers the user's question\n",
    "            - Provides specific details and examples\n",
    "            - Cites sources appropriately\n",
    "            - Acknowledges limitations in the information\n",
    "            \n",
    "            If search results are unavailable, acknowledge this limitation and provide \n",
    "            general information about the topic based on your knowledge.\n",
    "                        \n",
    "            {format_instructions}\n",
    "            \n",
    "            First analyze the query, then provide your response in the exact JSON format specified.\n",
    "            Your response should be informative, balanced, and tailored to the user's specific query.\"\"\"),\n",
    "            (\"human\", \"Analyze this search query: {query}\")\n",
    "        ])\n",
    "        \n",
    "        # Invoke the LLM to analyze the query with structured output\n",
    "        optimization_response = llm.invoke(\n",
    "            optimization_prompt.format_messages(\n",
    "                query=last_user_message,\n",
    "                format_instructions=format_instructions\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Parse the response into our Pydantic model\n",
    "        try:\n",
    "            structured_query = parser.parse(optimization_response.content)\n",
    "            \n",
    "            # Store the optimized query in the state\n",
    "            state.optimized_query = structured_query.optimized_query\n",
    "            \n",
    "            # You could store the full structured output in the state if needed\n",
    "            # state.structured_query_analysis = structured_query\n",
    "        except Exception as parse_error:\n",
    "            # Fallback handling if parsing fails\n",
    "            state.optimized_query = last_user_message\n",
    "        \n",
    "        return state\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Return the original state if there was an error\n",
    "        return state\n",
    "# STUDENT_IMPLEMENTATION_SECTION_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af87518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT_IMPLEMENTATION_SECTION_BEGIN\n",
    "def process_results_structured(state: AgentState) -> AgentState:\n",
    "    \"\"\"Process results with structured output parsing\"\"\"\n",
    "    try:\n",
    "        # Setup our Pydantic parser for the response\n",
    "        parser = PydanticOutputParser(pydantic_object=StructuredResponse)\n",
    "        format_instructions = parser.get_format_instructions()\n",
    "        \n",
    "        # Format search results for the LLM\n",
    "        search_results_text = \"\"\n",
    "        sources = []\n",
    "        \n",
    "        if not state.search_results:\n",
    "            search_results_text = \"No search results available. The search API may be unavailable.\"\n",
    "        else:\n",
    "            for result in state.search_results:\n",
    "                search_results_text += f\"Result {result.id}:\\nTitle: {result.title}\\nSnippet: {result.snippet}\\nSource: {result.link}\\n\\n\"\n",
    "                sources.append(result.link)\n",
    "                \n",
    "                if hasattr(result, 'full_content') and result.full_content:\n",
    "                    search_results_text += f\"\\nExtracted Content:\\n{result.full_content}\\n\\n\"\n",
    "        \n",
    "        # Create a prompt template for synthesis with structured output\n",
    "        synthesis_prompt = ChatPromptTemplate.from_messages([\n",
    "         (\"system\", \"\"\"You are an expert at extracting and summarizing relevant information from web content.\n",
    "                        \n",
    "                        Given a web page's content and a user query, your task is to:\n",
    "                        1. Identify the most relevant sections to the query\n",
    "                        2. Extract all the key facts, data points, and insights\n",
    "                        3. Organize the information in a clear, coherent structure\n",
    "                        4. Summarize the content while preserving accuracy\n",
    "                        5. Include any relevant technical details, statistics, or examples\n",
    "                        \n",
    "                        Focus on extracting ONLY information that addresses the user's query.\n",
    "                        Structure your summary with appropriate headings and bullet points if needed.\n",
    "                        Limit your response to the most important information (around 500-800 words).\n",
    "                        Make sure to follow this format:\n",
    "                        {format_instructions}\n",
    "                        First analyze the search results, then provide your response in the exact JSON format specified.\n",
    "          \n",
    "          \"\"\"),\n",
    "            (\"human\", \"User query: {original_query}\\n\\nSearch results:\\n{search_results}\"),\n",
    "        ])\n",
    "        \n",
    "        # Get the original query\n",
    "        original_query = next(\n",
    "            (msg.content for msg in state.messages if isinstance(msg, HumanMessage)),\n",
    "            \"Unknown query\"\n",
    "        )\n",
    "        \n",
    "        # Generate a structured response\n",
    "        response = llm.invoke(\n",
    "            synthesis_prompt.format_messages(\n",
    "                original_query=original_query,\n",
    "                search_results=search_results_text,\n",
    "                format_instructions=format_instructions\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Parse the structured response\n",
    "            structured_response = parser.parse(response.content)\n",
    "            \n",
    "            # Format the final response to the user with attribution\n",
    "            final_response = f\"{structured_response.answer}\\n\\n\"\n",
    "            \n",
    "            if structured_response.sources and len(structured_response.sources) > 0:\n",
    "                final_response += \"Sources:\\n\"\n",
    "                for source in structured_response.sources:\n",
    "                    final_response += f\"- {source}\\n\"\n",
    "            \n",
    "            if structured_response.related_topics and len(structured_response.related_topics) > 0:\n",
    "                final_response += \"\\nRelated topics you might be interested in:\\n\"\n",
    "                for topic in structured_response.related_topics:\n",
    "                    final_response += f\"- {topic}\\n\"\n",
    "                    \n",
    "            # Add confidence indicator\n",
    "            confidence_level = \"\"\n",
    "            if structured_response.confidence_score >= 0.8:\n",
    "                confidence_level = \"high confidence\"\n",
    "            elif structured_response.confidence_score >= 0.5:\n",
    "                confidence_level = \"moderate confidence\"\n",
    "            else:\n",
    "                confidence_level = \"low confidence\"\n",
    "                \n",
    "            final_response += f\"\\n(Response provided with {confidence_level})\"\n",
    "            \n",
    "            # Update the state with the assistant's response\n",
    "            state.messages.append(AIMessage(content=final_response))\n",
    "            \n",
    "        except Exception as parse_error:\n",
    "            # Fallback to raw response if parsing fails\n",
    "            state.messages.append(AIMessage(content=f\"I found some information that might help: {response.content}\"))\n",
    "        \n",
    "        return state\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Add a basic response if processing fails\n",
    "        state.messages.append(AIMessage(content=\"I apologize, but I encountered an issue processing your request. Please try again.\"))\n",
    "        return state\n",
    "# STUDENT_IMPLEMENTATION_SECTION_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaf0ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT_IMPLEMENTATION_SECTION_BEGIN\n",
    "# Create a workflow with structured output parsing\n",
    "structured_workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add our enhanced nodes\n",
    "structured_workflow.add_node(\"analyze\", analyze_query_structured)  # Structured query analysis\n",
    "structured_workflow.add_node(\"search\", perform_search)             # Reuse existing search\n",
    "structured_workflow.add_node(\"enrich\", enrich_content)             # Add the enrich node\n",
    "structured_workflow.add_node(\"process\", process_results_structured) # Structured response\n",
    "\n",
    "# Connect analyze to search\n",
    "structured_workflow.add_conditional_edges(\n",
    "    \"analyze\",\n",
    "    route_to_search,\n",
    "    {\n",
    "        \"search\": \"search\"  # Always go to search after analysis\n",
    "    }\n",
    ")\n",
    "\n",
    "# Connect search to enrich (use the should_enrich_content function)\n",
    "structured_workflow.add_conditional_edges(\n",
    "    \"search\",\n",
    "    should_enrich_content,\n",
    "    {\n",
    "        \"enrich\": \"enrich\",   # Enrich the top search result\n",
    "        \"process\": \"process\"  # Skip enrichment (not used in our simple flow)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Connect enrich to process\n",
    "structured_workflow.add_edge(\"enrich\", \"process\")\n",
    "\n",
    "# Connect process to end\n",
    "structured_workflow.add_conditional_edges(\n",
    "    \"process\",\n",
    "    is_done,\n",
    "    {\n",
    "        \"end\": END  # Always end after processing\n",
    "    }\n",
    ")\n",
    "\n",
    "# Set the entry point\n",
    "structured_workflow.set_entry_point(\"analyze\")\n",
    "\n",
    "# Compile the enhanced agent\n",
    "structured_agent = structured_workflow.compile()\n",
    "# STUDENT_IMPLEMENTATION_SECTION_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd566006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STUDENT_IMPLEMENTATION_SECTION_BEGIN\n",
    "# Test the structured workflow with a query\n",
    "query = \"Who is in 2025 NBA playoffs?\"\n",
    "initial_state = AgentState(messages=[HumanMessage(content=query)])\n",
    "\n",
    "# Run the enhanced agent with structured output parsing\n",
    "final_state = structured_agent.invoke(initial_state)\n",
    "\n",
    "# Display the conversation\n",
    "for message in final_state['messages']:\n",
    "    if isinstance(message, HumanMessage):\n",
    "        print(f\"Human: {message.content}\")\n",
    "    elif isinstance(message, AIMessage):\n",
    "        print(f\"AI: {message.content}\")\n",
    "    else:\n",
    "        print(message)\n",
    "# STUDENT_IMPLEMENTATION_SECTION_END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f249714c",
   "metadata": {},
   "source": [
    "## Benefits of Structured Output Parsing\n",
    "\n",
    "### Improved Agent Robustness\n",
    "\n",
    "As demonstrated above, using structured output parsing with Pydantic provides several advantages for our agent:\n",
    "\n",
    "1. **Clear Output Expectations**: The LLM knows exactly what format to produce\n",
    "2. **Schema Validation**: Automatic validation prevents malformed outputs  \n",
    "3. **Error Recovery**: Graceful fallbacks when parsing fails\n",
    "4. **Extensibility**: Easy to add new fields as requirements evolve\n",
    "\n",
    "### Real-World Applications\n",
    "\n",
    "Structured outputs are particularly valuable in production systems where:\n",
    "\n",
    "- Multiple services need to consume agent outputs\n",
    "- Responses need to be validated before downstream usage\n",
    "- You need consistent formatting across multiple LLM providers\n",
    "- Outputs must be logged or stored in databases\n",
    "\n",
    "The combination of LangGraph's structured state management and LangChain's output parsing capabilities creates a powerful foundation for building robust, maintainable agent systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
